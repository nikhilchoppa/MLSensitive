{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'XO8NmVeoGjHQh0TLVamD6ABmO'\n",
    "API_SECRET_KEY = 'aXYlbOAjis1eKPnXEhCU9iDQNzdYuMcJDXRHREA4o3ukn1ZKvJ'\n",
    "ACCESS_TOKEN = '943525676186869760-rYpA0V2LNEhmLPFlSn0tptCeO7zUtpg'\n",
    "ACCESS_SECRET = '59Ts2d33oxC7L4ft85kIzGDsTQQY5AEWPB2ZjtL54T8D3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is responsible for authenticating and creating an instance of the Twitter API using Tweepy. It \n",
    "initializes an OAuthHandler object with the API_KEY and API_SECRET_KEY, sets the access token using ACCESS_TOKEN and \n",
    "ACCESS_SECRET, and creates a Tweepy API instance with the given authentication details. This API instance is then \n",
    "returned and used throughout the script to fetch tweets.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d145d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_api():\n",
    "    auth = tweepy.OAuthHandler(API_KEY, API_SECRET_KEY)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3100a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function takes a raw tweet as input and cleans it by removing mentions, URLs, and non-alphanumeric characters \n",
    "using a regular expression. It then returns the cleaned tweet text as a string.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function fetches tweets from the Twitter API using the provided API instance, search query, and count (\n",
    "default is 1000). It filters tweets based on the query, language (English), tweet mode (extended for full tweet \n",
    "text), and tweets posted within a specified time window. The fetched tweets are then cleaned and stored in a list of \n",
    "dictionaries containing the cleaned tweet text, which is returned.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8077cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(api, query, count=1000):\n",
    "    tweets = []\n",
    "    until_datetime = datetime.now() - timedelta(hours=6)\n",
    "    until_str = until_datetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    fetched_tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", tweet_mode=\"extended\", until=until_str).items(\n",
    "        count)\n",
    "\n",
    "    for tweet in fetched_tweets:\n",
    "        parsed_tweet = {'text': clean_tweet(tweet.full_text)}\n",
    "        tweets.append(parsed_tweet)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function calls the get_tweets() function to fetch tweets and then labels them based on their sentiment using \n",
    "the NLTK SentimentIntensityAnalyzer. The polarity score for each cleaned tweet is determined using the analyzer, \n",
    "and the tweet is labeled as positive (2), neutral (1), or negative (0) based on the polarity score. The function \n",
    "returns a list of tuples, each containing the cleaned tweet text and its corresponding sentiment label.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_tweets(api, query, count=1000):\n",
    "    tweets = get_tweets(api, query, count)\n",
    "    cleaned_tweets = [clean_tweet(tweet['text']) for tweet in tweets]\n",
    "\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    labeled_tweets = []\n",
    "    for tweet in cleaned_tweets:\n",
    "        sentiment = sia.polarity_scores(tweet)['compound']\n",
    "        if sentiment >= 0.05:\n",
    "            labeled_tweets.append((tweet, 2))\n",
    "        elif sentiment <= -0.05:\n",
    "            labeled_tweets.append((tweet, 0))\n",
    "        else:\n",
    "            labeled_tweets.append((tweet, 1))\n",
    "\n",
    "    return labeled_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function builds a TensorFlow Sequential model for sentiment analysis. It takes a tokenizer and a number of \n",
    "labels as input and creates a model with an Embedding layer, GlobalAveragePooling1D layer, Dense layer with ReLU \n",
    "activation, and a final Dense layer with softmax activation for the sentiment labels. The model is compiled using the \n",
    "Adam optimizer, SparseCategoricalCrossentropy loss, and accuracy metric. The compiled model is then returned.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(tokenizer, labels_count):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=40),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(labels_count, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is responsible for training and saving the sentiment analysis model and tokenizer. It fetches \n",
    "labeled tweets using get_labeled_tweets(), tokenizes the tweet text, and pads the sequences. The training data is \n",
    "then split into training and test sets using a 67:33 ratio. The model is created, compiled, and trained using the \n",
    "create_model() function and early stopping callback. The trained model and tokenizer are then saved to disk as .h5 \n",
    "and .pickle files, respectively.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(query):\n",
    "    api = create_api()\n",
    "    labeled_tweets = get_labeled_tweets(api, query)\n",
    "    train_data = [tweet for tweet, label in labeled_tweets]\n",
    "    train_labels = [label for tweet, label in labeled_tweets]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "    padded_train_sequences = pad_sequences(train_sequences, maxlen=40, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_train_sequences, np.array(train_labels), test_size=0.33,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    model = create_model(tokenizer, labels_count=3)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=10, callbacks=[early_stopping])\n",
    "\n",
    "    model_file = f'{query}_model.h5'\n",
    "    tokenizer_file = f'{query}_tokenizer.pickle'\n",
    "\n",
    "    model.save(model_file)\n",
    "\n",
    "    with open(tokenizer_file, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bd9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function loads a pre-trained model and tokenizer from disk and predicts the sentiment of a given input text. \n",
    "If the required model and tokenizer files do not exist, the function calls train_and_save_model() to train and save a \n",
    "new model. The input text is tokenized and padded using the loaded tokenizer, and the sentiment prediction is made \n",
    "using the loaded model. The function returns the predicted sentiment probabilities.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ecc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(query, text):\n",
    "    model_file = f'{query}_model.h5'\n",
    "    tokenizer_file = f'{query}_tokenizer.pickle'\n",
    "\n",
    "    if not os.path.exists(model_file) or not os.path.exists(tokenizer_file):\n",
    "        train_and_save_model(query)\n",
    "\n",
    "    with open(tokenizer_file, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=40, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    return model.predict(padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This part of the script is responsible for executing the entire sentiment analysis process. It starts by prompting \n",
    "the user to input the stock or crypto symbol they wish to analyze. Then, it fetches tweets related to the given \n",
    "symbol using the get_tweets() function. For each fetched tweet, the script cleans the tweet text, predicts its \n",
    "sentiment using the predict_sentiment() function, and updates the sentiments dictionary accordingly.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9770cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter the stock or crypto symbol: \").lower()\n",
    "tweets = get_tweets(create_api(), query, count=100)\n",
    "sentiments = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734cc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    text = clean_tweet(tweet['text'])\n",
    "    sentiment_prediction = predict_sentiment(query, text)\n",
    "    sentiment_label = sentiment_prediction.argmax(axis=-1)[0]\n",
    "\n",
    "    if sentiment_label == 0:\n",
    "        sentiments[\"negative\"] += 1\n",
    "    elif sentiment_label == 1:\n",
    "        sentiments[\"neutral\"] += 1\n",
    "    else:\n",
    "        sentiments[\"positive\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(sentiments.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_percentage = sentiments[\"positive\"] / total * 100\n",
    "negative_percentage = sentiments[\"negative\"] / total * 100\n",
    "neutral_percentage = sentiments[\"neutral\"] / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sentiment analysis for {query}')\n",
    "print(f'Positive: {positive_percentage:.2f}%')\n",
    "print(f'Negative: {negative_percentage:.2f}%')\n",
    "print(f'Neutral: {neutral_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca57098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pie chart\n",
    "sizes = [positive_percentage, negative_percentage, neutral_percentage]\n",
    "labels = ['Positive', 'Negative', 'Neutral']\n",
    "colors = ['blue', '#FF4500', '#B0C4DE']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNote: This sentiment analysis might not accurately capture sarcasm or nuanced expressions of sentiment.')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
